# awesome-neural-networks

> Last Update: [  ]

This repository is a dive into vanilla neural networks, implemented in pure NumPy to accelerate foundational learning.

It's designed for anyone with a base level of knowledge on Calculus, Linear Algebra, and Programming to gain an understanding of Neural Networks from the first principles.

## motivation

Neural Networks, in the context of artifiical intelligence, have been powering LLMs ([GPT-4o](https://openai.com/index/hello-gpt-4o/), [Claude-3](https://www.anthropic.com/news/claude-3-family), [Gemini](https://deepmind.google/technologies/gemini/), [Mixtral](https://mistral.ai/news/mixtral-of-experts/), etc) and leading the charge towards progressing to a hypothetical artifical general intelligence. 

The possible futures humanity can unlock with these models are increasingly optimistic and they prove to be an invaluable tool, for those who are inspired to accelerate the coming of artifical intelligence, to learn from first principles.

Personally, I knew that if I wanted to get deeper into the technical trenches of convolutional neural networks, transformers, deep RL, and more, foundational knowledge was key to understand and refine.

So this is why I built this repo, to build foundational knowledge for myself and build a resource for others to do so as well.

## table of contents

1. [Logistic & Softmax Regression](#softmax-regression)




---

## Outline

**CURRENT AREA OF FOCUS: Softmax Regression**

### 1. A Brief History of Neural Networks
- [ ] Write an overview of the history and evolution of neural networks.

### 2. Logistic SoftMax Regression
- [X] Recap on Logistic Regression
  - [X] Logistic Mathematical Function
  - [X] Affine Transformation
  - [X] Introduce Concept of a Neuron / Perceptron
  - [X] Forward Pass
  - [X] Loss Function
  - [X] Backward Pass
  - [X] Weight Update
  - [X] Gradient Descent
  - [X] Implement Logistic Regression.
  - [X] Implement and explain SoftMax regression.
  - [X] An Intuition Bridging Logistic and Softmax regression
  - [X] Forward Pass
  - [X] Backward Pass

### 3. Vanilla Neural Networks
#### 3.1 Overview of Neural Network Flow
- [ ] Describe the base mathematics and process.
  - [ ] Explain Forward Pass.
  - [ ] Explain Backward Pass.
  - [ ] Explain Weight Update.

#### 3.2 Different Activation Functions
- [ ] Implement and explain Sigmoid.
- [ ] Implement and explain TanH.
- [ ] Implement and explain ReLU.
- [ ] Implement and explain ELU.
- [ ] Implement and explain SELU.

#### 3.3 One Hot Encoding
- [ ] Implement and explain One Hot Encoding.

#### 3.4 Loss Functions
- [ ] Implement and explain Categorical Cross Entropy.
- [ ] Implement and explain Smoothed Categorical Cross Entropy.

#### 3.5 Softmax as Output
- [ ] Implement and explain Softmax as an output layer.

#### 3.6 Neural Networks with Weight Initialization Methods
- [ ] Implement and explain Xavier Initialization.
- [ ] Implement and explain He Initialization.

#### 3.7 Full Neural Network Flow Explanation and Code
- [ ] Provide a full explanation and code for the complete neural network flow.

### 4. Neural Networks with Mini-Batch Descent
- [ ] Implement and explain neural networks with Mini-Batch Gradient Descent.

### 5. Neural Networks with Regularization
#### 5.1 L1 Regularization
- [ ] Implement and explain L1 Regularization.

#### 5.2 L2 Regularization
- [ ] Implement and explain L2 Regularization.

#### 5.3 Dropout
- [ ] Implement and explain Dropout.

### 6. Neural Networks with Scheduled Learning Rates
#### 6.1 Halving
- [ ] Implement and explain learning rate halving.

#### 6.2 Exponential Decay
- [ ] Implement and explain exponential decay of learning rate.

#### 6.3 Cyclical Learning
- [ ] Implement and explain cyclical learning rates.

#### 6.4 1cycle & Superconvergence
- [ ] Implement and explain 1cycle policy and superconvergence.

### 7. Neural Networks with Advanced Optimization
#### 7.1 Momentum
- [ ] Implement and explain Momentum optimization.

#### 7.2 RMSprop
- [ ] Implement and explain RMSprop optimization.

#### 7.3 Adam
- [ ] Implement and explain Adam optimization.

#### 7.4 AdaMax
- [ ] Implement and explain AdaMax optimization.

### 8. Neural Networks with Batch Normalization
- [ ] Implement and explain Batch Normalization.

### 9. Resources
- [ ] Compile a list of recommended books, research papers, online courses, tutorials, and blogs.

### 10. How to Contribute
- [ ] Write guidelines on how to contribute to the repository.

### 11. MIT License
- [ ] Add the MIT License to the repository.
