A Git Repo on Neural Networks -- all implemented from Scratch

# Neural Networks from Scratch

## Outline

### 1. A Brief History of Neural Networks
- [ ] Write an overview of the history and evolution of neural networks.

### 2. SoftMax Regression
- [ ] Implement and explain SoftMax regression.

### 3. Vanilla Neural Networks
#### 3.1 Overview of Neural Network Flow
- [ ] Describe the base mathematics and process.
  - [ ] Explain Forward Pass.
  - [ ] Explain Backward Pass.
  - [ ] Explain Weight Update.

#### 3.2 Different Activation Functions
- [ ] Implement and explain Sigmoid.
- [ ] Implement and explain TanH.
- [ ] Implement and explain ReLU.
- [ ] Implement and explain ELU.
- [ ] Implement and explain SELU.

#### 3.3 One Hot Encoding
- [ ] Implement and explain One Hot Encoding.

#### 3.4 Loss Functions
- [ ] Implement and explain Categorical Cross Entropy.
- [ ] Implement and explain Smoothed Categorical Cross Entropy.

#### 3.5 Softmax as Output
- [ ] Implement and explain Softmax as an output layer.

#### 3.6 Neural Networks with Weight Initialization Methods
- [ ] Implement and explain Xavier Initialization.
- [ ] Implement and explain He Initialization.

#### 3.7 Full Neural Network Flow Explanation and Code
- [ ] Provide a full explanation and code for the complete neural network flow.

### 4. Neural Networks with Mini-Batch Descent
- [ ] Implement and explain neural networks with Mini-Batch Gradient Descent.

### 5. Neural Networks with Regularization
#### 5.1 L1 Regularization
- [ ] Implement and explain L1 Regularization.

#### 5.2 L2 Regularization
- [ ] Implement and explain L2 Regularization.

#### 5.3 Dropout
- [ ] Implement and explain Dropout.

### 6. Neural Networks with Scheduled Learning Rates
#### 6.1 Halving
- [ ] Implement and explain learning rate halving.

#### 6.2 Exponential Decay
- [ ] Implement and explain exponential decay of learning rate.

#### 6.3 Cyclical Learning
- [ ] Implement and explain cyclical learning rates.

#### 6.4 1cycle & Superconvergence
- [ ] Implement and explain 1cycle policy and superconvergence.

### 7. Neural Networks with Advanced Optimization
#### 7.1 Momentum
- [ ] Implement and explain Momentum optimization.

#### 7.2 RMSprop
- [ ] Implement and explain RMSprop optimization.

#### 7.3 Adam
- [ ] Implement and explain Adam optimization.

#### 7.4 AdaMax
- [ ] Implement and explain AdaMax optimization.

### 8. Neural Networks with Batch Normalization
- [ ] Implement and explain Batch Normalization.

### 9. Resources
- [ ] Compile a list of recommended books, research papers, online courses, tutorials, and blogs.

### 10. How to Contribute
- [ ] Write guidelines on how to contribute to the repository.

### 11. MIT License
- [ ] Add the MIT License to the repository.
